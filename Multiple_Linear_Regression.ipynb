{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SyURsEG4hgii",
        "outputId": "e817c722-260f-4ad6-a013-a3496501ad7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3DM_T1mZl2RX",
        "outputId": "95517598-3ace-477d-f5c3-68933e7109dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features:  tensor([[ 7., 99.,  1.,  9.,  1.],\n",
            "        [ 4., 82.,  0.,  4.,  2.],\n",
            "        [ 8., 51.,  1.,  7.,  2.],\n",
            "        ...,\n",
            "        [ 6., 83.,  1.,  8.,  5.],\n",
            "        [ 9., 97.,  1.,  7.,  0.],\n",
            "        [ 7., 74.,  0.,  8.,  1.]])\n",
            "Features size:  torch.Size([10000, 5])\n",
            "\n",
            "Target:  tensor([[91.],\n",
            "        [65.],\n",
            "        [45.],\n",
            "        ...,\n",
            "        [74.],\n",
            "        [95.],\n",
            "        [64.]])\n",
            "Target size:  torch.Size([10000, 1])\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"Student_Performance.csv\") # Reading and saving dataframe\n",
        "features = [\"Hours Studied\", \"Previous Scores\", \"Extracurricular Activities\", \"Sleep Hours\", \"Sample Question Papers Practiced\"]\n",
        "target = \"Performance Index\"\n",
        "\n",
        "df[\"Extracurricular Activities\"] = df[\"Extracurricular Activities\"].map({'Yes': 1, 'No': 0})\n",
        "X = df[features].values\n",
        "y = df[target].values.reshape(len(df[target].values), 1) # Equivalent y = df[target].values.reshape(-1, 1)\n",
        "X = torch.from_numpy(X).float()\n",
        "y = torch.from_numpy(y).float()\n",
        "print(\"Features: \", X)\n",
        "print(\"Features size: \", X.shape)\n",
        "print()\n",
        "print(\"Target: \", y)\n",
        "print(\"Target size: \", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Gt4BiTSnWQR",
        "outputId": "640c10d1-ab20-443f-a51c-e204b1fa0c1e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8000, 8000, 2000, 2000)\n"
          ]
        }
      ],
      "source": [
        "train_split = int(len(X) * 0.8)\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "print((len(X_train), len(y_train), len(X_test), len(y_test)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akUQe-Dsrc6D",
        "outputId": "ba716658-46dd-452d-93e6-fb010f905b9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('w1', tensor([0.3367])),\n",
              "             ('w2', tensor([0.1288])),\n",
              "             ('w3', tensor([0.2345])),\n",
              "             ('w4', tensor([0.2303])),\n",
              "             ('w5', tensor([-1.1229])),\n",
              "             ('bias', tensor([-0.1863]))])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "class LinearRegression(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.w1 = nn.Parameter(torch.randn(1,\n",
        "                                    requires_grad=True,\n",
        "                                    dtype=torch.float))\n",
        "    self.w2 = nn.Parameter(torch.randn(1,\n",
        "                                    requires_grad=True,\n",
        "                                    dtype=torch.float))\n",
        "    self.w3 = nn.Parameter(torch.randn(1,\n",
        "                                    requires_grad=True,\n",
        "                                    dtype=torch.float))\n",
        "    self.w4 = nn.Parameter(torch.randn(1,\n",
        "                                    requires_grad=True,\n",
        "                                    dtype=torch.float))\n",
        "    self.w5 = nn.Parameter(torch.randn(1,\n",
        "                                    requires_grad=True,\n",
        "                                    dtype=torch.float))\n",
        "    self.bias = nn.Parameter(torch.randn(1,\n",
        "                                    requires_grad=True,\n",
        "                                    dtype=torch.float))\n",
        "  def forward(self, hrs, prevScores, extraCurry, sleepHrs, sampleQp):\n",
        "    return hrs * self.w1 + prevScores * self.w2 + extraCurry * self.w3 + sleepHrs * self.w4 + sampleQp * self.w5 + self.bias\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model_0 = LinearRegression()\n",
        "model_0.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "9r0LoDhhtxGI"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(params=model_0.parameters(),\n",
        "                            lr=0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "g1NGGETJQCwG",
        "outputId": "a04ccf3d-0633-499d-f46b-b3d2347bbd0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([8000, 1])) that is different to the input size (torch.Size([8000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([2000, 1])) that is different to the input size (torch.Size([2000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Train loss: 3061.93408203125 | Test loss: 3065.53759765625\n",
            "Epoch: 10 | Train loss: 2883.225341796875 | Test loss: 2885.189208984375\n",
            "Epoch: 20 | Train loss: 2712.14208984375 | Test loss: 2712.671630859375\n",
            "Epoch: 30 | Train loss: 2549.40771484375 | Test loss: 2548.683349609375\n",
            "Epoch: 40 | Train loss: 2395.379638671875 | Test loss: 2393.5498046875\n",
            "Epoch: 50 | Train loss: 2250.114990234375 | Test loss: 2247.304931640625\n",
            "Epoch: 60 | Train loss: 2113.47998046875 | Test loss: 2109.797119140625\n",
            "Epoch: 70 | Train loss: 1985.2340087890625 | Test loss: 1980.7752685546875\n",
            "Epoch: 80 | Train loss: 1865.086181640625 | Test loss: 1859.9398193359375\n",
            "Epoch: 90 | Train loss: 1752.724365234375 | Test loss: 1746.971923828125\n",
            "Epoch: 100 | Train loss: 1647.825927734375 | Test loss: 1641.544189453125\n",
            "Epoch: 110 | Train loss: 1550.068115234375 | Test loss: 1543.3277587890625\n",
            "Epoch: 120 | Train loss: 1459.1275634765625 | Test loss: 1451.994140625\n",
            "Epoch: 130 | Train loss: 1374.6834716796875 | Test loss: 1367.2178955078125\n",
            "Epoch: 140 | Train loss: 1296.41845703125 | Test loss: 1288.677001953125\n",
            "Epoch: 150 | Train loss: 1224.0185546875 | Test loss: 1216.0526123046875\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-808823188.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# 2. Compute loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m   \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;31m# 3. Optimizer zero grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3887\u001b[0m             )\n\u001b[1;32m   3888\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3889\u001b[0;31m         return torch._C._nn.mse_loss(\n\u001b[0m\u001b[1;32m   3890\u001b[0m             \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3891\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "  model_0.train()\n",
        "  # 1. Forward pass\n",
        "  train_preds = model_0.forward(X_train[:, 0], X_train[:, 1], X_train[:, 2], X_train[:, 3], X_train[:, 4])\n",
        "\n",
        "  # 2. Compute loss\n",
        "  train_loss = loss_fn(train_preds, y_train)\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. loss backward\n",
        "  train_loss.backward()\n",
        "\n",
        "  # 5. Optimizer step\n",
        "  optimizer.step()\n",
        "\n",
        "  model_0.eval()\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    test_preds = model_0.forward(X_test[:, 0], X_test[:, 1], X_test[:, 2], X_test[:, 3], X_test[:, 4])\n",
        "    test_loss = loss_fn(test_preds, y_test)\n",
        "\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch} | Train loss: {train_loss} | Test loss: {test_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_Ih_NfhRyW2",
        "outputId": "7f5f2168-5c91-42a7-ef88-24cc8fd0d5ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([16.1072])\n"
          ]
        }
      ],
      "source": [
        "with torch.inference_mode():\n",
        "  print(model_0.forward(torch.tensor(7), torch.tensor(99), torch.tensor(1), torch.tensor(9), torch.tensor(1)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2yaWc2WUT6Tr"
      },
      "outputs": [],
      "source": [
        "class LinearRegressionUpd(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(in_features=5, out_features=1)\n",
        "\n",
        "  def forward(self, X: torch.tensor):\n",
        "    return self.linear(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3208c02",
        "outputId": "542f634d-d4d2-40cf-e996-b54373287d59"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('linear.weight',\n",
              "              tensor([[ 0.3419,  0.3712, -0.1048,  0.4108, -0.0980]])),\n",
              "             ('linear.bias', tensor([0.0902]))])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "model_1 = LinearRegressionUpd()\n",
        "model_1.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(params=model_1.parameters(),\n",
        "                             lr=0.01)"
      ],
      "metadata": {
        "id": "PlarctQOPyZh"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1000\n",
        "for epoch in range(epochs):\n",
        "  model_1.train()\n",
        "  # 1. Forward pass\n",
        "  train_preds = model_1(X_train)\n",
        "  # 2. Compute loss\n",
        "  train_loss = loss_fn(train_preds, y_train)\n",
        "\n",
        "  # 3. Zero gradient\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Backward propagation\n",
        "  train_loss.backward()\n",
        "\n",
        "  # 5. Optimizer step\n",
        "  optimizer.step()\n",
        "\n",
        "  model_1.eval()\n",
        "  with torch.inference_mode():\n",
        "    test_preds = model_1(X_test)\n",
        "    test_loss = loss_fn(test_preds, y_test)\n",
        "  if epoch % 10 == 0:\n",
        "    print(f\"Epoch: {epoch} | Train loss: {train_loss} | Test loss: {test_loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faixCppXQNe_",
        "outputId": "9aac5d8c-96a2-4049-9d2f-02452d18b64c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Train loss: 4.128206253051758 | Test loss: 4.244961261749268\n",
            "Epoch: 10 | Train loss: 4.1378583908081055 | Test loss: 4.241723537445068\n",
            "Epoch: 20 | Train loss: 4.133723735809326 | Test loss: 4.261905193328857\n",
            "Epoch: 30 | Train loss: 4.128113746643066 | Test loss: 4.252127170562744\n",
            "Epoch: 40 | Train loss: 4.128567695617676 | Test loss: 4.2453107833862305\n",
            "Epoch: 50 | Train loss: 4.128189563751221 | Test loss: 4.245049953460693\n",
            "Epoch: 60 | Train loss: 4.128085136413574 | Test loss: 4.2473931312561035\n",
            "Epoch: 70 | Train loss: 4.128075122833252 | Test loss: 4.248076438903809\n",
            "Epoch: 80 | Train loss: 4.128062725067139 | Test loss: 4.246922016143799\n",
            "Epoch: 90 | Train loss: 4.128054618835449 | Test loss: 4.2470197677612305\n",
            "Epoch: 100 | Train loss: 4.128055572509766 | Test loss: 4.247386932373047\n",
            "Epoch: 110 | Train loss: 4.128055095672607 | Test loss: 4.24706506729126\n",
            "Epoch: 120 | Train loss: 4.128055095672607 | Test loss: 4.247253894805908\n",
            "Epoch: 130 | Train loss: 4.128054618835449 | Test loss: 4.247133731842041\n",
            "Epoch: 140 | Train loss: 4.128054618835449 | Test loss: 4.247211456298828\n",
            "Epoch: 150 | Train loss: 4.128054618835449 | Test loss: 4.247183799743652\n",
            "Epoch: 160 | Train loss: 4.128054618835449 | Test loss: 4.2471842765808105\n",
            "Epoch: 170 | Train loss: 4.128054618835449 | Test loss: 4.247213840484619\n",
            "Epoch: 180 | Train loss: 4.128054618835449 | Test loss: 4.247218608856201\n",
            "Epoch: 190 | Train loss: 4.128054618835449 | Test loss: 4.247254371643066\n",
            "Epoch: 200 | Train loss: 4.12805700302124 | Test loss: 4.247480392456055\n",
            "Epoch: 210 | Train loss: 4.128148078918457 | Test loss: 4.249234199523926\n",
            "Epoch: 220 | Train loss: 4.134399890899658 | Test loss: 4.271258354187012\n",
            "Epoch: 230 | Train loss: 4.131996154785156 | Test loss: 4.240769386291504\n",
            "Epoch: 240 | Train loss: 4.12865686416626 | Test loss: 4.247585296630859\n",
            "Epoch: 250 | Train loss: 4.128772258758545 | Test loss: 4.251959800720215\n",
            "Epoch: 260 | Train loss: 4.128054618835449 | Test loss: 4.24835729598999\n",
            "Epoch: 270 | Train loss: 4.12815523147583 | Test loss: 4.245914936065674\n",
            "Epoch: 280 | Train loss: 4.128054618835449 | Test loss: 4.24683952331543\n",
            "Epoch: 290 | Train loss: 4.128066539764404 | Test loss: 4.247763633728027\n",
            "Epoch: 300 | Train loss: 4.128057479858398 | Test loss: 4.247001647949219\n",
            "Epoch: 310 | Train loss: 4.128054618835449 | Test loss: 4.247136116027832\n",
            "Epoch: 320 | Train loss: 4.128054618835449 | Test loss: 4.247264385223389\n",
            "Epoch: 330 | Train loss: 4.128054618835449 | Test loss: 4.247119426727295\n",
            "Epoch: 340 | Train loss: 4.128054618835449 | Test loss: 4.247223854064941\n",
            "Epoch: 350 | Train loss: 4.128054618835449 | Test loss: 4.247168064117432\n",
            "Epoch: 360 | Train loss: 4.128054141998291 | Test loss: 4.247168064117432\n",
            "Epoch: 370 | Train loss: 4.128054618835449 | Test loss: 4.24718713760376\n",
            "Epoch: 380 | Train loss: 4.128054618835449 | Test loss: 4.247191905975342\n",
            "Epoch: 390 | Train loss: 4.128054618835449 | Test loss: 4.2471795082092285\n",
            "Epoch: 400 | Train loss: 4.128054141998291 | Test loss: 4.247196197509766\n",
            "Epoch: 410 | Train loss: 4.128054618835449 | Test loss: 4.247270107269287\n",
            "Epoch: 420 | Train loss: 4.1280646324157715 | Test loss: 4.247836112976074\n",
            "Epoch: 430 | Train loss: 4.129108428955078 | Test loss: 4.25556755065918\n",
            "Epoch: 440 | Train loss: 4.134471416473389 | Test loss: 4.253552436828613\n",
            "Epoch: 450 | Train loss: 4.131589889526367 | Test loss: 4.260746479034424\n",
            "Epoch: 460 | Train loss: 4.128323078155518 | Test loss: 4.253077507019043\n",
            "Epoch: 470 | Train loss: 4.128082752227783 | Test loss: 4.248348236083984\n",
            "Epoch: 480 | Train loss: 4.128188133239746 | Test loss: 4.246291160583496\n",
            "Epoch: 490 | Train loss: 4.128114223480225 | Test loss: 4.24600076675415\n",
            "Epoch: 500 | Train loss: 4.128055095672607 | Test loss: 4.246744632720947\n",
            "Epoch: 510 | Train loss: 4.128062725067139 | Test loss: 4.247469425201416\n",
            "Epoch: 520 | Train loss: 4.128055572509766 | Test loss: 4.247414588928223\n",
            "Epoch: 530 | Train loss: 4.128055572509766 | Test loss: 4.247069358825684\n",
            "Epoch: 540 | Train loss: 4.128054618835449 | Test loss: 4.247138977050781\n",
            "Epoch: 550 | Train loss: 4.128054618835449 | Test loss: 4.2472405433654785\n",
            "Epoch: 560 | Train loss: 4.128054618835449 | Test loss: 4.247152805328369\n",
            "Epoch: 570 | Train loss: 4.128054618835449 | Test loss: 4.247197151184082\n",
            "Epoch: 580 | Train loss: 4.128054618835449 | Test loss: 4.24717378616333\n",
            "Epoch: 590 | Train loss: 4.128054618835449 | Test loss: 4.247189998626709\n",
            "Epoch: 600 | Train loss: 4.128055095672607 | Test loss: 4.247178077697754\n",
            "Epoch: 610 | Train loss: 4.128054618835449 | Test loss: 4.24718713760376\n",
            "Epoch: 620 | Train loss: 4.128054618835449 | Test loss: 4.247186660766602\n",
            "Epoch: 630 | Train loss: 4.128054618835449 | Test loss: 4.247159004211426\n",
            "Epoch: 640 | Train loss: 4.128054141998291 | Test loss: 4.247125148773193\n",
            "Epoch: 650 | Train loss: 4.128055095672607 | Test loss: 4.247028350830078\n",
            "Epoch: 660 | Train loss: 4.12807035446167 | Test loss: 4.246433734893799\n",
            "Epoch: 670 | Train loss: 4.128890037536621 | Test loss: 4.242717266082764\n",
            "Epoch: 680 | Train loss: 4.137506484985352 | Test loss: 4.240799427032471\n",
            "Epoch: 690 | Train loss: 4.128055095672607 | Test loss: 4.243773937225342\n",
            "Epoch: 700 | Train loss: 4.129244804382324 | Test loss: 4.2524094581604\n",
            "Epoch: 710 | Train loss: 4.128069877624512 | Test loss: 4.249287128448486\n",
            "Epoch: 720 | Train loss: 4.128206729888916 | Test loss: 4.245721340179443\n",
            "Epoch: 730 | Train loss: 4.128054618835449 | Test loss: 4.246750831604004\n",
            "Epoch: 740 | Train loss: 4.128071308135986 | Test loss: 4.247900485992432\n",
            "Epoch: 750 | Train loss: 4.128060340881348 | Test loss: 4.246906757354736\n",
            "Epoch: 760 | Train loss: 4.128055095672607 | Test loss: 4.247204303741455\n",
            "Epoch: 770 | Train loss: 4.128054141998291 | Test loss: 4.247223854064941\n",
            "Epoch: 780 | Train loss: 4.128054618835449 | Test loss: 4.2471513748168945\n",
            "Epoch: 790 | Train loss: 4.128054618835449 | Test loss: 4.24718713760376\n",
            "Epoch: 800 | Train loss: 4.128054618835449 | Test loss: 4.2472147941589355\n",
            "Epoch: 810 | Train loss: 4.128054141998291 | Test loss: 4.247149467468262\n",
            "Epoch: 820 | Train loss: 4.128054618835449 | Test loss: 4.247167110443115\n",
            "Epoch: 830 | Train loss: 4.128054618835449 | Test loss: 4.247159957885742\n",
            "Epoch: 840 | Train loss: 4.128054618835449 | Test loss: 4.247126579284668\n",
            "Epoch: 850 | Train loss: 4.128055095672607 | Test loss: 4.24702262878418\n",
            "Epoch: 860 | Train loss: 4.128075122833252 | Test loss: 4.246328353881836\n",
            "Epoch: 870 | Train loss: 4.129411697387695 | Test loss: 4.241916179656982\n",
            "Epoch: 880 | Train loss: 4.132335662841797 | Test loss: 4.243628978729248\n",
            "Epoch: 890 | Train loss: 4.129662036895752 | Test loss: 4.24153995513916\n",
            "Epoch: 900 | Train loss: 4.1283040046691895 | Test loss: 4.247073650360107\n",
            "Epoch: 910 | Train loss: 4.128453254699707 | Test loss: 4.250462055206299\n",
            "Epoch: 920 | Train loss: 4.128054618835449 | Test loss: 4.24809455871582\n",
            "Epoch: 930 | Train loss: 4.128108978271484 | Test loss: 4.246220588684082\n",
            "Epoch: 940 | Train loss: 4.128055572509766 | Test loss: 4.247002601623535\n",
            "Epoch: 950 | Train loss: 4.128059387207031 | Test loss: 4.247602462768555\n",
            "Epoch: 960 | Train loss: 4.12805700302124 | Test loss: 4.2469940185546875\n",
            "Epoch: 970 | Train loss: 4.128055095672607 | Test loss: 4.247221946716309\n",
            "Epoch: 980 | Train loss: 4.128054618835449 | Test loss: 4.247201919555664\n",
            "Epoch: 990 | Train loss: 4.128055095672607 | Test loss: 4.247163772583008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "final_pred = model_1(X).detach().numpy()\n",
        "table2 = np.column_stack((y,final_pred))\n",
        "\n",
        "print(tabulate(table2[:5], headers=(\"Actual_Data\",\"Data_Predicted\"), tablefmt=\"fancy_grid\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aC4HOQMBUnfc",
        "outputId": "156d6bf0-944c-46a3-dd96-4f79153f1a73"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "╒═══════════════╤══════════════════╕\n",
            "│   Actual_Data │   Data_Predicted │\n",
            "╞═══════════════╪══════════════════╡\n",
            "│            91 │          91.8244 │\n",
            "├───────────────┼──────────────────┤\n",
            "│            65 │          63.1528 │\n",
            "├───────────────┼──────────────────┤\n",
            "│            45 │          45.0539 │\n",
            "├───────────────┼──────────────────┤\n",
            "│            36 │          36.5691 │\n",
            "├───────────────┼──────────────────┤\n",
            "│            66 │          67.052  │\n",
            "╘═══════════════╧══════════════════╛\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}